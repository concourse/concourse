#language anatomy

\use{\load{concourse/docs}}

\template{\load{concourse/docs-template}}

\title{Installing}{installing}

So you want to get yourself a Concourse.

There are a few supported ways to get going. All distributions of Concourse are
equivalent in feature set, the only difference being operational concerns like
scaling up your workers, and how quickly you can bootstrap.

For the quickest introduction, you may want to spin up a
\reference{vagrant}{local VM with Vagrant}. This should take only a few
minutes, depending on how quickly you can download the image.

Once you're ready to install Concourse somewhere more proper, have a look at
the \reference{binaries}{standalone binary} or the
\reference{docker-repository}{Docker repository}.

Or, if you want to invest a bit in spinning up a fully automated cluster using
a tool that meshes well with Concourse's ideals, buckle up and head over to
\reference{clusters-with-bosh}.

\split-sections

\section{Local VM with Vagrant}{vagrant}{
  \omit-children-from-table-of-contents

  The quickest way to spin up a fully functioning Concourse is with
  \hyperlink{https://vagrantup.com}{Vagrant}.

  Just run the following in any directory:

  \codeblock{bash}{
  vagrant init concourse/lite # creates ./Vagrantfile
  vagrant up                  # downloads the box and spins up the VM
  }

  The web server will be running at
  \hyperlink{http://192.168.100.4:8080}{192.168.100.4:8080}.

  While this isn't exactly production ready, it may be enough depending on your
  project's needs. Given that Concourse is stateless, you can always hoist your
  pipeline onto a bigger installation when you're ready, so there's little risk
  in sticking with the Vagrant boxes while you figure things out.

  \section{Upgrading}{
    Note that upgrading Concourse in Vagrant unfortunately requires re-creating
    the machine, which will wipe out all of your submitted pipelines, build
    history, etc. If Vagrant gives a warning of \code{A newer version of the box 'concourse/lite' is available!},
    run the following:

    \codeblock{bash}{
    vagrant box update --box concourse/lite # gets the newest Vagrant box
    vagrant destroy                         # remove the old Vagrant box
    vagrant up                              # re-create the machine with the newer box
    }
  }
}

\section{Standalone Binary}{binaries}{
  \omit-children-from-table-of-contents

  At some point you may want to start putting Concourse on to real hardware. A
  binary distribution is available in the \reference{downloads}{downloads}
  section.

  The binary is fairly self-contained, making it ideal for tossing onto a VM by
  hand or orchestrating it with Docker, Chef, or other ops tooling.

  \section{Prerequisites}{
    \list{
      Grab the appropriate binary for your platform from the
      \reference{downloads}{downloads} section.
    }{
      For Linux you'll need kernel v3.19 or later, with user namespace support
      enabled. Windows and Darwin don't really need anything special.
    }{
      PostgresSQL 9.3+
    }
  }

  \section{Generating Keys}{generating-keys}{
    To run Concourse securely you'll need to generate 3 private keys (well, 2,
    plus 1 for each worker):

    \definitions{
      \item{\code{session_signing_key} (currently must be RSA)}{
        Used for signing user session tokens, and by the TSA to sign its own
        tokens in the requests it makes to the ATC.
      }

      \item{\code{host_key}}{
        Used for the TSA's SSH server. This is the key whose fingerprint you
        see when the \code{ssh} command warns you when connecting to a host it
        hasn't seen before.
      }

      \item{\code{worker_key} (one per worker)}{
        Used for authorizing worker registration. There can actually be an
        arbitrary number of these keys; they are just listed to authorize
        worker SSH access.
      }
    }

    To generate these keys, run:

    \codeblock{bash}{
    ssh-keygen -t rsa -f host_key -N ''
    ssh-keygen -t rsa -f worker_key -N ''
    ssh-keygen -t rsa -f session_signing_key -N ''
    }

    ...and we'll also start on an \code{authorized_keys} file, currently
    listing this initial worker key:

    \codeblock{bash}{
    cp worker_key.pub authorized_worker_keys
    }
  }

  \section{Starting the Web UI & Scheduler}{
    The \code{concourse} binary embeds the
    \hyperlink{https://github.com/concourse/atc}{ATC} and
    \hyperlink{https://github.com/concourse/tsa}{TSA} components, available as
    the \code{web} subcommand.

    The ATC is the component responsible for scheduling builds, and also serves
    as the web UI and API.

    The TSA provides a SSH interface for securely registering workers, even if
    they live in their own private network.

    \section{Single node, local Postgres}{
      The following command will spin up the ATC, listening on port
      \code{8080}, with some basic auth configured, and a TSA listening on port
      \code{2222}.

      \codeblock{bash}{
      concourse web \\
        --basic-auth-username myuser \\
        --basic-auth-password mypass \\
        --session-signing-key session_signing_key \\
        --tsa-host-key host_key \\
        --tsa-authorized-keys authorized_worker_keys \\
        --external-url http://my-ci.example.com
      }

      This assumes you have a local Postgres server running on the default port
      (\code{5432}) with an \code{atc} database, accessible by the current
      user. If your database lives elsewhere, just specify the
      \code{--postgres-data-source} flag, which is also demonstrated below.

      Be sure to replace the \code{--external-url} flag with the URI you expect
      to use to reach your Concourse server.

      In the above example we've configured basic auth for the
      \reference{main-team}{\code{main} team}. For further configuration see
      \reference{authentication}.
    }

    \section{Cluster with remote Postgres}{
      The ATC can be scaled up for high availability, and they'll also roughly
      share their scheduling workloads, using the database to synchronize.

      The TSA can also be scaled up, and requires no database as there's no
      state to synchronize (it just talks to the ATC).

      A typical configuration with multiple ATC+TSA nodes would have them
      sitting behind a load balancer, forwarding port \code{80} to \code{8080},
      \code{443} to \code{4443} (if you've enabled TLS), and \code{2222} to
      \code{2222}.

      To run multiple \code{web} nodes, you'll need to pass the following
      flags:

      \list{
        \code{--postgres-data-source} should all refer to the same database
      }{
        \code{--peer-url} should be a URL used to reach the individual ATC,
        from other ATCs, i.e. a URL usable within their private network
      }{
        \code{--external-url} should be the URL used to reach \italic{any} ATC,
        i.e. the URL to your load balancer
      }

      For example:

      Node 0:

      \codeblock{bash}{
      concourse web \\
        --basic-auth-username myuser \\
        --basic-auth-password mypass \\
        --session-signing-key session_signing_key \\
        --tsa-host-key host_key \\
        --tsa-authorized-keys authorized_worker_keys \\
        --postgres-data-source postgres://user:pass@10.0.32.0/concourse \\
        --external-url https://ci.example.com \\
        --peer-url http://10.0.16.10:8080
      }

      Node 1 (only difference is \code{--peer-url}):

      \codeblock{bash}{
      concourse web \\
        --basic-auth-username myuser \\
        --basic-auth-password mypass \\
        --session-signing-key session_signing_key \\
        --tsa-host-key host_key \\
        --tsa-authorized-keys authorized_worker_keys \\
        --postgres-data-source postgres://user:pass@10.0.32.0/concourse \\
        --external-url https://ci.example.com \\
        --peer-url http://10.0.16.11:8080
      }
    }
  }

  \section{Starting Workers}{standalone-workers}{
    Workers are
    \hyperlink{http://github.com/cloudfoundry-incubator/garden}{Garden}
    servers, continuously heartbeating their presence to the Concourse API.
    Workers have a statically configured \code{platform} and a set of
    \code{tags}, both of which determine where steps in a
    \reference{build-plans}{Build Plan} are scheduled.

    You may want a few workers, depending on the resource usage of your
    pipeline. There should be one per machine; running multiple on one box
    doesn't really make sense, as each worker runs as many containers as
    Concourse requests of it.

    To spin up a worker and register it with your Concourse cluster running
    locally, run:

    \codeblock{bash}{
    sudo concourse worker \\
      --work-dir /opt/concourse/worker \\
      --tsa-host 127.0.0.1 \\
      --tsa-public-key host_key.pub \\
      --tsa-worker-private-key worker_key
    }

    Note that the worker must be run as \code{root}, as it orchestrates
    containers.

    The \code{--work-dir} flag specifies where container data should be placed;
    make sure it has plenty of disk space available, as it's where all the disk
    usage across your builds and resources will end up.

    The \code{--tsa-host} refers to wherever your TSA node is listening, by
    default on port \code{2222} (pass \code{--tsa-port} if you've configured it
    differently). This may be an address to a load balancer if you're running
    multiple \code{web} nodes, or just an IP, perhaps \code{127.0.0.1} if
    you're running everything on one box.

    The \code{--tsa-public-key} flag is used to ensure we're connecting to the
    TSA we should be connecting to, and is used like \code{known_hosts} with
    the \code{ssh} command. Refer to \reference{generating-keys} if you're not
    sure what this means.

    The \code{--tsa-worker-private-key} flag specifies the key to use when
    authenticating to the TSA. Refer to \reference{generating-keys} if you're
    not sure what this means.
  }
}

\section{Docker Repository}{docker-repository}{
  The \reference{binaries}{standalone binary} is available in an official
  Docker repository: \code{concourse/concourse}. There are tags for each
  version, including release candidates if you want to live on the bleeding
  edge.

  For a quick start with \hyperlink{https://docs.docker.com/compose/}{Docker
  Compose}, create \code{docker-compose.yml}:

  \titled-codeblock{docker-compose.yml}{yaml}{
  concourse-db:
    image: postgres:9.5
    environment:
      POSTGRES_DB: concourse
      POSTGRES_USER: concourse
      POSTGRES_PASSWORD: changeme
      PGDATA: /database

  concourse-web:
    image: concourse/concourse
    links: [concourse-db]
    command: web
    ports: ["8080:8080"]
    volumes: ["./keys/web:/concourse-keys"]
    environment:
      CONCOURSE_BASIC_AUTH_USERNAME: concourse
      CONCOURSE_BASIC_AUTH_PASSWORD: changeme
      CONCOURSE_EXTERNAL_URL: "$\{CONCOURSE_EXTERNAL_URL\}"
      CONCOURSE_POSTGRES_DATA_SOURCE: |-
        postgres://concourse:changeme@concourse-db:5432/concourse?sslmode=disable

  concourse-worker:
    image: concourse/concourse
    privileged: true
    links: [concourse-web]
    command: worker
    volumes: ["./keys/worker:/concourse-keys"]
    environment:
      CONCOURSE_TSA_HOST: concourse-web
  }

  Then, run the following to generate the necessary keys:

  \codeblock{bash}{
    mkdir -p keys/web keys/worker

    ssh-keygen -t rsa -f ./keys/web/tsa_host_key -N ''
    ssh-keygen -t rsa -f ./keys/web/session_signing_key -N ''

    ssh-keygen -t rsa -f ./keys/worker/worker_key -N ''

    cp ./keys/worker/worker_key.pub ./keys/web/authorized_worker_keys
    cp ./keys/web/tsa_host_key.pub ./keys/worker
  }

  The next thing you'll need is an address that can be used to reach the
  \code{web} node from within your network. This is the
  \code{$CONCOURSE_EXTERNAL_URL} variable. It \italic{can't} be
  \code{127.0.0.1} or \code{localhost} as it has to also work in a separate
  network namespace, for \reference{fly-execute} to be able to receive your
  uploaded bits from the \code{web} node.

  If you're using \code{docker-machine}, set \code{CONCOURSE_EXTERNAL_URL} to
  whatever the machine's IP is, for example:

  \codeblock{bash}{
    export CONCOURSE_EXTERNAL_URL=http://192.168.99.100:8080
  }

  Then, spin everything up:

  \codeblock{bash}{
    docker-compose up
  }

  Next, browse to your configured external URL (probably
  \hyperlink{http://127.0.0.1:8080}{127.0.0.1:8080} or
  \hyperlink{http://192.168.99.100:8080}{192.168.99.100:8080}) and log in with
  the username \code{concourse} and password \code{changeme}.

  Given that the Docker repository simply wraps the binary, you'll want to
  reference \reference{binaries}{the binary documentation} or just run
  \code{docker run concourse/concourse <command> --help} if you want to learn
  how to do any further configuration.

  Every flag can also be configured via environment variables in the form of
  \code{CONCOURSE_(flag)}, where \code{(flag)} is the uppercased flag name with
  all hyphens conveted to underscores. For example,
  \code{--basic-auth-username} would be \code{CONCOURSE_BASIC_AUTH_USERNAME}.

  In the above example we've configured basic auth for the
  \reference{main-team}{\code{main} team}. For further configuration see
  \reference{authentication}.
}

\section{Clusters with BOSH}{clusters-with-bosh}{
  \omit-children-from-table-of-contents

  Deploying Concourse with \hyperlink{https://bosh.io}{BOSH} provides a
  scalable cluster with health management and rolling upgrades.

  If you're not yet familiar with BOSH, learning it will be a bit of an
  investment, but it should pay off in spades. There are a lot of parallels
  between the philosophy of BOSH and Concourse.

  \section{Prepping the Environment}{prepping-bosh}{
    To go from nothing to a BOSH managed Concourse, you'll need to do the following:

    \list{
      \hyperlink{http://bosh.io/docs/init.html}{Initialize a BOSH director},
      which is the server that orchestrates BOSH deployments.
    }{
      \hyperlink{http://bosh.io/docs/cloud-config.html}{Set up your Cloud
      Config}, which describes the infrastructure to which Concourse will be
      deployed.
    }{
      \hyperlink{http://bosh.io/docs/uploading-stemcells.html}{Upload the
      stemcell}, which contains the base image used for VMs managed by BOSH.
    }

    You can skip some of this if you already have a working BOSH director
    running BOSH v255.4 and up.
  }

  \section{Deploying Concourse}{
    Once you've got all that set up, download the releases listed for your
    version of Concourse from the \reference{downloads} section, and
    \hyperlink{http://bosh.io/docs/uploading-releases.html}{upload them to the
    BOSH director}.

    Next you'll need a Concourse
    \hyperlink{http://bosh.io/docs/deployment.html}{BOSH deployment manifest}. An
    example manifest is below; you'll want to replace the \code{REPLACE_ME} bits
    with whatever values are appropriate.

    Note that the VM types, VM extensions, persistent disk type, and network names
    must come from your Cloud Config. Consult \reference{prepping-bosh} if you
    haven't set it up yet. You can retrieve your Cloud Config by running
    \code{bosh cloud-config}.

    \codeblock{yaml}{
      ---
      name: concourse

      # replace with `bosh status --uuid`
      director_uuid: REPLACE_ME

      releases:
      - name: concourse
        version: latest
      - name: garden-runc
        version: latest

      stemcells:
      - alias: trusty
        os: ubuntu-trusty
        version: latest

      instance_groups:
      - name: web
        instances: 1
        # replace with a VM type from your BOSH Director's cloud config
        vm_type: REPLACE_ME
        vm_extensions:
        # replace with a VM extension from your BOSH Director's cloud config that will attach
        # this instance group to your ELB
        - REPLACE_ME
        stemcell: trusty
        azs: [z1]
        networks: [\{name: private\}]
        jobs:
        - name: atc
          release: concourse
          properties:
            # replace with your CI's externally reachable URL, e.g. https://ci.foo.com
            external_url: REPLACE_ME

            # replace with username/password, or configure GitHub auth
            basic_auth_username: REPLACE_ME
            basic_auth_password: REPLACE_ME

            # replace with your SSL cert and key
            tls_cert: REPLACE_ME
            tls_key: REPLACE_ME

            postgresql_database: &atc_db atc
        - name: tsa
          release: concourse
          properties: \{\}

      - name: db
        instances: 1
        # replace with a VM type from your BOSH Director's cloud config
        vm_type: REPLACE_ME
        stemcell: trusty
        # replace with a disk type from your BOSH Director's cloud config
        persistent_disk_type: REPLACE_ME
        azs: [z1]
        networks: [\{name: private\}]
        jobs:
        - name: postgresql
          release: concourse
          properties:
            databases:
            - name: *atc_db
              # make up a role and password
              role: REPLACE_ME
              password: REPLACE_ME

      - name: worker
        instances: 1
        # replace with a VM type from your BOSH Director's cloud config
        vm_type: REPLACE_ME
        vm_extensions:
        # replace with a VM extension from your BOSH Director's cloud config that will attach
        # sufficient ephemeral storage to VMs in this instance group.
        - REPLACE_ME
        stemcell: trusty
        azs: [z1]
        networks: [\{name: private\}]
        jobs:
        - name: groundcrew
          release: concourse
          properties: \{\}
        - name: baggageclaim
          release: concourse
          properties: \{\}
        - name: garden
          release: garden-runc
          properties:
            garden:
              listen_network: tcp
              listen_address: 0.0.0.0:7777

      update:
        canaries: 1
        max_in_flight: 1
        serial: false
        canary_watch_time: 1000-60000
        update_watch_time: 1000-60000
    }

    You may also want to consult the property descriptions for your version of
    Concourse at
    \hyperlink{https://bosh.io/releases/github.com/concourse/concourse}{bosh.io}
    to see what properties you can/should tweak.

    You may also want to consult \reference{authentication} for configuring
    something other than basic auth for the \reference{main-team}{\code{main}
    team}.

    Once you've got a manifest, just
    \hyperlink{https://bosh.io/docs/deploying.html}{deploy it}!
  }

  \section{Reaching the web UI}{
    This really depends on your infrastructure. If you're deploying to AWS you
    may want to configure the \code{web} VM type to register with an ELB,
    mapping port \code{80} to \code{8080}, \code{443} to \code{4443} (if you've
    configured TLS), and \code{2222} to \code{2222}.

    Otherwise you may want to configure \code{static_ips} for the \code{web}
    instance group and just reach the web UI directly.
  }

  \section{Upgrading & maintaining Concourse}{
    With BOSH, the deployment manifest is the source of truth. This is very
    similar to Concourse's own philosophy, where all pipeline configuration is
    defined in a single declarative document.

    So, to add more workers or web nodes, just change the \code{instances}
    value for the instance group and re-run \code{bosh deploy}.

    To upgrade, just upload the new releases and re-run \code{bosh deploy}.
  }

  \section{Supporting external workers}{configuring-bosh-tsa}{
    If you need workers that run outside of your BOSH managed deployment (e.g.
    for testing with iOS or in some special network), you'll need to make some
    tweaks to the default configuration of the \code{tsa} job.

    The \reference{architecture-tsa} is the entryway for workers to join the
    cluster. For every new worker key pair, the TSA will be told to authorize
    its public key, and the workers must also know the TSA's public key ahead
    of time, so they know who they're connecting to.

    \section{Configuring the TSA's host key}{
      First you'll need to remove the "magic" from the default deployment. By
      default, Concourse generates a key pair for the TSA, and gives the public
      key to the workers so they can trust the connection. This key
      occasionally gets cycled, which is fine so long as things are in one
      deployment, but once you have external workers you would have to update
      them all manually, which is annoying.

      To fix this, generate a passwordless key pair via \code{ssh-keygen}, and
      provide the following properties in your BOSH deployment manifest:

      \definitions{
        \item{\code{host_key} on the \code{tsa} job}{
          the contents of the private key for the TSA server
        }
        \item{\code{host_public_key} on the \code{tsa} job}{
          the public key of the TSA, for the workers to use to verify the
          connection
        }
      }

      For example (note that this manifest omits a bunch of stuff):

      \codeblock{yaml}{
      instance_groups:
      - name: web
        jobs:
        - name: tsa
          release: concourse
          properties:
            host_key: |
              -----BEGIN RSA PRIVATE KEY-----
              <a bunch of stuff>
              -----END RSA PRIVATE KEY-----
            host_public_key: "ssh-rsa blahblahblah"
        - # ...
      }

      After setting these properties, be sure to run \code{bosh deploy}.
    }

    \section{Authorizing worker keys}{
      We've left the worker keys auto-generated so far, which is fine for
      workers deployed alongside the TSA, as it'll also automatically authorize
      them.

      External workers however will need their own private keys, and so the TSA
      must be told to authorize them.

      To do so, set the following properties:

      \definitions{
        \item{\code{authorized_keys} on the \code{tsa} job}{
          the array of public keys to authorize
        }
        \item{\code{tsa.private_key} on the \code{groundcrew} job}{
          the private key for the worker to use when accessing the TSA
        }
        \item{\code{tsa.host} and \code{tsa.host_public_key} on the \code{groundcrew} job}{
          if the worker is in a separate deployment, these must be configured
          to reach the TSA
        }
      }

      Once again, after setting these properties run \code{bosh deploy} to make
      the changes take place.
    }

    \section{Making the TSA reachable}{
      Typically the TSA and ATC will both be colocated in the same instance
      group. This way a single load balancer can be used with the following
      scheme:

      \list{
        expose port \code{443} to \code{8080} (ATC's HTTP port) via SSL
      }{
        expose port \code{2222} to \code{2222} (TSA's SSH port) via TCP
      }

      Be sure to update any relevant security group rules (or equivalent in
      non-AWS environments) to permit both access from the outside world to
      port \code{2222} on your load balancer, \italic{and} access from the load
      balancer to port \code{2222} on your TSA + ATC instances.

      The BOSH deployment manifest would then colocate both jobs together, like
      so:

      \codeblock{yaml}{
      instance_groups:
      - name: web
        vm_type: web_lb
        jobs:
        - name: atc
          release: concourse
          # ...
        - name: tsa
          release: concourse
          # ...
      }

      In AWS, the \code{web_lb} VM type would then configure
      \code{cloud_properties.elbs} to auto-register instances of \code{web}
      with an ELB. See
      \hyperlink{https://bosh.io/docs/aws-cpi.html#resource-pools}{the AWS CPI
      docs} for more information.
    }
  }
}
