#language anatomy

\title{Deploying with BOSH}{deploying-with-bosh}

Once you start needing more workers and \italic{really} caring about your CI
deployment, you may want to consider deploying and managing Concourse with
\hyperlink{https://bosh.io}{BOSH}.

Using BOSH gives you a self-healing, predictable environment that can be
scaled up or down just by changing numbers in a \code{.yml} file and running
\code{bosh deploy}. It's not only for spinning up the initial deployment, but
for managing it and upgrading it the next day.

If you grok Concourse and agree with a lot of its opinions, then you'll
probably enjoy BOSH, though the learning curve may seem a bit steep at first.
That being said, you can deploy Concourse however you want; it doesn't require
BOSH or integrate with it at all once it's running.

The \hyperlink{https://github.com/concourse/concourse}{Concourse repo} is a
\hyperlink{https://github.com/cloudfoundry/bosh}{BOSH} release containing
everything necessary to deploy to arbitrary infrastructures (or your laptop)
with largely the same configuration.

The \hyperlink{https://bosh.io/docs}{BOSH documentation} outlines the
concepts, and how to bootstrap on various infrastructures.

\table-of-contents

\section{Setting up the infrastructure}{
  Step one is to pick your infrastructure. AWS, vSphere, and OpenStack are
  fully supported by BOSH.
  \hyperlink{https://github.com/cloudfoundry/bosh-lite}{BOSH-Lite} is a
  pseudo-infrastructure that deploys everything within a single VM. It is a
  great way to get started with Concourse and BOSH at the same time, with a
  faster feedback loop.

  Concourse's infrastructure requirements are fairly straightforward. For
  example, Concourse's own pipeline is deployed within an AWS VPC, with its web
  instances automatically registered with an Elastic Load Balancer by BOSH.

  \hyperlink{http://consul.io}{Consul} is baked into the BOSH release, so that
  you only need to configure static IPs for the Consul servers, and then
  configure the Consul agents on the other jobs to join with the server. This
  way you can have 100 workers without having to configure them 100 times.

  \section{BOSH-Lite}{
    Learning BOSH and Concourse at the same time will probably be hard. If
    you're getting started with BOSH, you may want to check out
    \hyperlink{https://github.com/cloudfoundry/bosh-lite}{BOSH-Lite} first,
    which gives you a fairly BOSHy experience, in a single VM on your machine.
    This is a good way to learn the BOSH tooling without having to pay for
    costly infrastructure.
  }

  \section{AWS}{
    For AWS, it is recommended to deploy Concourse within a VPC, with the
    \code{web} jobs sitting behind an ELB. Registering instances with the ELB
    is automated by BOSH; you'll just have to create the ELB itself. This
    configuration is more secure, as your CI system's internal jobs aren't
    exposed to the outside world.

    To get this all set up:
    \list{
      Use our
      \hyperlink{https://github.com/concourse/concourse/blob/master/manifests/cloudformation.json}{CloudFormation
      template}, which will set up a VPC, ELB and an elastic IP for a BOSH
      director, with the appropriate security groups/subnets. 
      Note: Alternatively, you could replicate a similar setup after examining
      the template without having to use CloudFormation. 
      
    }{
      Follow along with the (skipping step 2 if you used CloudFormation)
      \hyperlink{http://bosh.io/docs/init-aws.html}{\code{bosh-init} AWS docs}.
    }{
      Correctly configure SSL for the Elastic Load Balancer see: 
      \hyperlink{http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/ssl-server-cert.html}{SSL for AWS ELB Setup}.
    }{
      Test your installation by checking the AWS assigned DNS for the Load 
      Balancer and navigating to that address in your web browser. 
    }
  }

  \section{vSphere, OpenStack, etc.}{
    As long as BOSH supports your infrastructure, you should be able to deploy
    Concourse to it, in largely the same way.

    To get everything set up, reference the \code{bosh-init} bootstrapping docs
    for your infrastructure at \hyperlink{http://bosh.io/docs}{bosh.io}.
  }
}


\section{Deploying Concourse}{
  Once you've set up BOSH on your infrastructure, the following steps should
  get you started:

  \section{Upload the stemcell}{
    A stemcell is a base image for your VMs. It controls the kernel and OS
    distribution, and the version of the BOSH agent. For more information,
    consult the \hyperlink{http://bosh.io/docs/stemcell.html}{BOSH
    documentation}.

    To upload the latest AWS Trusty stemcell to your BOSH director, execute the
    following:

    \codeblock{bash}{
    bosh upload stemcell https://bosh.io/d/stemcells/bosh-aws-xen-hvm-ubuntu-trusty-go_agent
    }
  }

  \section{Upload the releases}{
    A release is a curated distribution of all of the source bits and
    configuration necessary to deploy and scale a product. To learn more about
    BOSH releases, refer to the
    \hyperlink{http://bosh.io/docs/release.html}{BOSH documentation}.

    A Concourse deployment currently requires two releases:
    \hyperlink{http://github.com/concourse/concourse}{Concourse}'s release
    itself, and
    \hyperlink{http://github.com/cloudfoundry-incubator/garden-linux-release}{Garden
    Linux}, which it uses for container management.

    These are two independently-versioned products, so your best bet for
    deploying the correct version combination is to go to our
    \hyperlink{https://github.com/concourse/concourse/releases/latest}{latest
    GitHub release} and grab Concourse and Garden-Linux from the
    \italic{Downloads} section.

    For example, to upload v0.54.0 of Concourse with the appropriate Garden
    Linux release, you would run:

    \codeblock{bash}{
    bosh upload release https://github.com/concourse/concourse/releases/download/v0.54.0/concourse-0.54.0.tgz
    bosh upload release https://github.com/concourse/concourse/releases/download/v0.54.0/garden-linux-0.205.0.tgz
    }

    \italic{Don't just copy these! These versions are really old. Get the latest
    from the GitHub link above.}
  }

  \section{Configure & Deploy}{bosh-properties}{
    All you need to deploy your entire Concourse cluster is a BOSH deployment
    manifest. This single document describes the desired layout of an entire
    cluster.

    The Concourse repo contains a few example manifests:

    \list{
      \hyperlink{https://github.com/concourse/concourse/blob/master/manifests/bosh-lite.yml}{BOSH Lite}
    }{
      \hyperlink{https://github.com/concourse/concourse/blob/master/manifests/aws-vpc.yml}{AWS VPC}
    }

    If you reuse these manifests, you'll probably want to change them around
    first. Consult the docs for the various Concourse release jobs at
    \hyperlink{http://bosh.io/releases/github.com/concourse/concourse}{bosh.io}
    for more information about the properties to set. For everything else, e.g.
    \code{resource_pools} and other non-Concourse-specific configuration,
    consult the rest of BOSH's \hyperlink{http://bosh.io/docs}{documentation}.
    
    The one thing that you'll \bold{definitely} need to add to your manifest
    is basic authentication credentials. The server will fail to start without
    an authentication mechanism set. We don't provide any defaults here to stop
    someone accidentally deploying with those defaults and then never changing
    them. The properties are called \code{atc.basic_auth_username} and
    \code{atc.basic_auth_password}.

    Once you have a deployment manifest, deploying Concourse should simply be:

    \codeblock{bash}{
    $ bosh deployment path/to/manifest.yml
    $ bosh deploy
    }
  }
}

\section{Upgrading & Maintenance}{
  Compared to other deployment schemes, the main strength of BOSH its its
  ability to upgrade arbitrary clusters and keep them healthy after the initial
  deploy, even as your infrastructure may be experiencing some turbulence.

  \section{Updating the BOSH deployment}{
    When new Concourse versions come out, upgrading should simply be a matter of
    uploading the new releases
    \hyperlink{https://github.com/concourse/concourse/releases/latest}{from
      GitHub} and \code{bosh deploy}ing again. BOSH will then kick off a rolling
    deploy of your cluster. Be sure to read the \reference{release-notes} first
    though!

    You can change the values in your manifest (such as the number of workers)
    at any time and \code{bosh deploy}, and BOSH will do The Right Thingâ„¢. It
    will tear down VMs as necessary, but always make sure any persistent data
    persists, and that everything's in its right place at the end.
  }

  \section{Self-upgrading Concourse}{
    You can also use Concourse to upgrade itself, using the
    \hyperlink{https://github.com/concourse/concourse/blob/master/ci/pipelines/concourse-redeploy.yml}{concourse-redeploy}
    pipeline configuration. Note that the build that's deploying Concourse from
    itself may fail or error, as it may literally tear down the VM running the
    deploy. But don't worry - all the real work is being done by the BOSH
    director, so just keep an eye on it using \code{bosh task}!
  }

  \section{Dealing with turbulence}{
    Things go wrong all the time. BOSH is built to be durable to things like
    faulty VMs or infrastructure-level failures, and should be able to bring
    your cluster back into the desired state.

    This is often done automatically since BOSH health monitors its VMs and
    will resurrect them (if that's enabled). If you see something wrong and
    want to take action on it immediately, you can run \code{bosh cck} to look
    for errors and fix them. You can also run \code{bosh recreate} to tear down
    an instance and bring up a new one, if you suspect it's gone awry.

    If all else fails, the stateless nature of Concourse should make it pretty
    easy to just nuke the site from orbit (\code{bosh delete deployment}) and
    deploy it all over again. You'll lose build history if you're using the
    built-in \code{postgres} server, but everything should start up again once
    things are back.
  }
}
