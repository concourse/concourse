\title{\aux{Running a }\code{web} node}{concourse-web}{web-node}

\use-plugin{concourse-docs}
\omit-children-from-table-of-contents

The \code{web} node is responsible for running the web UI, API, and as well
as performing all pipeline scheduling. It's basically the brain of Concourse.

\section{
  \title{Prerequisites}{web-prerequisites}

  Nothing special - the \code{web} node is a pretty simple Go application that
  can be run like a 12-factor app.
}

\section{
  \title{Running}{web-running}

  The \code{concourse} CLI can run as a \code{web} node via the \code{web}
  subcommand.

  Before running it, let's configure a local user so we can log in:

  \codeblock{bash}{{{
  CONCOURSE_ADD_LOCAL_USER=myuser:mypass
  CONCOURSE_MAIN_TEAM_LOCAL_USER=myuser
  }}}

  This will configure a single user, \code{myuser}, with the password
  \code{mypass}. You'll probably want to change those to sensible values, and
  later you may want to configure a proper auth provider - check out
  \reference{auth} whenever you're ready.

  Next, you'll need to configure the session signing key, the SSH key for the
  worker gateway, and the authorized worker key. Check
  \reference{generating-keys} to learn what these are and how they are created.

  \codeblock{bash}{{{
  CONCOURSE_SESSION_SIGNING_KEY=path/to/session_signing_key
  CONCOURSE_TSA_HOST_KEY=path/to/tsa_host_key
  CONCOURSE_TSA_AUTHORIZED_KEYS=path/to/authorized_worker_keys
  }}}

  Finally, \code{web} needs to know how to reach your Postgres database. This
  can be set like so:

  \codeblock{bash}{{{
  CONCOURSE_POSTGRES_HOST=127.0.0.1 # default
  CONCOURSE_POSTGRES_PORT=5432      # default
  CONCOURSE_POSTGRES_DATABASE=atc   # default
  CONCOURSE_POSTGRES_USER=my-user
  CONCOURSE_POSTGRES_PASSWORD=my-password
  }}}

  If you're running PostgreSQL locally, you can probably just point it to the
  socket and rely on the \code{peer} auth:

  \codeblock{bash}{{{
  CONCOURSE_POSTGRES_SOCKET=/var/run/postgresql
  }}}

  Now that everything's set, run:

  \codeblock{bash}{{{
  concourse web
  }}}

  All logs will be emitted to \code{stdout}, with any panics or lower-level
  errors being emitted to \code{stderr}.

  \section{
    \title{Ingress}{web-ingress}

    If your web nodes are going to be accessed over the network, you will need
    to set \code{CONCOURSE_EXTERNAL_URL} to a URL accessible by your consumers.
    If you don't set this property, logins will redirect to its default value of
    \code{127.0.0.1}.

    If your instance is available on the public internet, you may wish to
    prevent the Concourse UI from being nefariously embedded as an iframe by
    setting \code{CONCOURSE_X_FRAME_OPTIONS} to \code{deny} (to prevent
    \italic{any} iframe embeddings) or \code{sameorigin} (to only allow iframe
    embeddings in pages served from the same subdomain). This protects against
    clickjacking.

    \bold{Note:} If setting the value to \code{allow-from}, please note that not
    all browsers support this value and when not supported, the header is ignored
    by the browser.
  }
}

\section{
  \title{Properties}{web-properties}

  CPU usage: peaks during pipeline scheduling, primarily when scheduling
  \reference{pipeline-jobs}. Mitigated by adding more \code{web} nodes. In this
  regard, \code{web} nodes can be considered compute-heavy more than anything
  else at large scale.

  Memory usage: not very well classified at the moment as it's not generally a
  concern. Give it a few gigabytes and keep an eye on it.

  Disk usage: none

  Bandwidth usage: aside from handling external traffic, the \code{web} node
  will at times have to stream bits out from one worker and into another while
  executing \reference{steps}.

  Highly available: yes; \code{web} nodes can all be configured the same (aside
  from \code{--peer-address}) and placed behind a load balancer. Periodic tasks
  like garbage-collection will not be duplicated for each node.

  Horizontally scalable: yes; they will coordinate workloads using the
  database, resulting in less work for each node and thus lower CPU usage.

  Outbound traffic:

  \list{
    \code{db} on its configured port for persistence
  }{
    \code{db} on its configured port for locking and coordinating in a
    multi-\code{web} node deployment
  }{
    directly-registered \code{worker} nodes on ports \code{7777}, \code{7788},
    and \code{7799} for checking \reference{pipeline-resources}, executing
    builds, and performing garbage-collection
  }{
    other \code{web} nodes (possibly itself) on an ephemeral port when a worker
    is forwarded through the web node's TSA
  }

  Inbound traffic:

  \list{
    \code{worker} connects to the TSA on port \code{2222} for registration
  }{
    \code{worker} downloads inputs from the ATC during \reference{fly-execute}
    via its external URL
  }{
    external traffic to the ATC API via the web UI and \reference{fly-cli}
  }
}

\section{
  \title{Operation}{web-operation}

  The \code{web} nodes themselves are stateless - they don't store anything on
  disk, and coordinate entirely using the database.

  \section{
    \title{Scaling}

    The \reference{web-node} can be scaled up for high availability. They'll also
    roughly share their scheduling workloads, using the database to synchronize.
    This is done by just running more \code{web} commands on different machines,
    and optionally putting them behind a load balancer.

    To run a cluster of \reference{web-node}s, you'll first need to ensure
    they're all pointing to the same PostgreSQL server.

    Furthermore, you may wish to configure the max number of parallel database
    connections that each node makes. The number you choose should not be
    greater than the maximum number of simultaneous connections your Postgres
    server will allow. For example:

    \codeblock{bash}{{{
    CONCOURSE_MAX_OPEN_CONNECTIONS=32
    }}}

    Next, you'll need to configure a \italic{peer address}. This is a DNS or IP
    address that can be used to reach this \code{web} node from other
    \code{web} nodes. Typically this uses a private IP, like so:

    \codeblock{bash}{{{
    CONCOURSE_PEER_ADDRESS=10.10.0.1
    }}}

    This address will be used for forwarded worker connections, which listen on
    the ephemeral port range.

    Finally, if all of these nodes are going to be accessed through a load
    balancer, you'll need to configure the external URL that will be used to
    reach your Concourse cluster:

    \codeblock{bash}{{{
    CONCOURSE_EXTERNAL_URL=https://ci.example.com
    }}}

    Aside from the peer URL, all configuration must be consistent across all
    \code{web} nodes in the cluster to ensure consistent results.
  }

  \section{
    \title{Restarting & Upgrading}

    The \code{web} nodes can be killed and restarted willy-nilly. No draining
    is necessary; if the \code{web} node was orchestrating a build it will just
    continue where it left off when it comes back, or the build will be
    picked up by one of the other \code{web} nodes.

    To upgrade a \code{web} node, stop its process and start a new one using
    the newly installed \code{concourse}. Any migrations will be run
    automatically on start. If \code{web} nodes are started in parallel, only
    one will run the migrations.

    Note that we don't currently guarantee a lack of funny-business if you're
    running mixed Concourse versions - database migrations can perform
    modifications that confuse other \code{web} nodes. So there may be some
    turbulence during a rolling upgrade, but everything should stabilize once
    all \code{web} nodes are running the latest version.
  }

  \section{
    \title{Downgrading}

    If you're stuck in a pinch and need to downgrade from one version of
    Concourse to another, you can use the \code{concourse migrate} command.

    \italic{
      Note: support for down migrations is a fairly recent addition to
      Concourse; it is not supported for downgrading to v3.6.0 and below.
    }

    First, grab the desired migration version by running the following:

    \codeblock{bash}{{{
    # make sure this is the *old* Concourse binary
    $ concourse migrate --supported-db-version
    1551110547
    }}}

    That number (yours will be different) is the expected migration version for
    that version of Concourse.

    Next, run the following with the \italic{new} Concourse binary:

    \codeblock{bash}{{{
    $ concourse migrate --migrate-db-to-version=1551110547
    }}}

    This will need the same \code{CONCOURSE_POSTGRES_*} configuration described
    in \reference{web-running}.

    Once this completes, switch all \code{web} nodes back to the older
    \code{concourse} binary and you should be good to go.
  }
}

\section{
  \title{Configuration}{web-configuration}

  \section{
    \title{Giving your cluster a name}

    If you've got many Concourse clusters that you switch between, you can make
    it slightly easier to notice which one you're on by giving each cluster a
    name:

    \codeblock{bash}{{{
    CONCOURSE_CLUSTER_NAME=production
    }}}

    When set, this name will be shown in the top bar when viewing the dashboard.
  }

  \section{
    \title{TLS via Let's Encrypt}{lets-encrypt}

    Concourse can be configured to automatically acquire a TLS certificate via
    \link{Let's Encrypt}{https://letsencrypt.org}:

    \codeblock{bash}{{{
      # Enable TLS
      CONCOURSE_TLS_BIND_PORT=443

      # Enable Let's Encrypt
      CONCOURSE_ENABLE_LETS_ENCRYPT=true
    }}}

    \warn{
      Concourse's Let's Encrypt integration works by storing the TLS
      certificate and key in the database, so it is imperative that you enable
      \reference{encryption}{database encryption} as well.
    }

    By default, Concourse will reach out to \link{Let's Encrypt's ACME CA
    directory}{https://acme-v01.api.letsencrypt.org/directory}. An alernative
    URL can be configured like so:

    \codeblock{bash}{{{
      CONCOURSE_LETS_ENCRYPT_ACME_URL=https://acme.example.com/directory
    }}}

    In order to negotiate the certificate, your \code{web} node must be
    reachable by the ACME server. There are intentionally \link{no publicly
    listed IP addresses to whitelist}{https://letsencrypt.org/docs/faq/}, so
    this typically means just making your \code{web} node publicly reachable.
  }

  \section{
    \title{Enabling audit logs}{audit-logs}

    A very simplistic form of audit logging can be enabled with the following
    vars:

    \codeblock{bash}{{{
      # Enable auditing for all api requests connected to builds.
      CONCOURSE_ENABLE_BUILD_AUDITING=true

      # Enable auditing for all api requests connected to containers.
      CONCOURSE_ENABLE_CONTAINER_AUDITING=true

      # Enable auditing for all api requests connected to jobs.
      CONCOURSE_ENABLE_JOB_AUDITING=true

      # Enable auditing for all api requests connected to pipelines.
      CONCOURSE_ENABLE_PIPELINE_AUDITING=true

      # Enable auditing for all api requests connected to resources.
      CONCOURSE_ENABLE_RESOURCE_AUDITING=true

      # Enable auditing for all api requests connected to system transactions.
      CONCOURSE_ENABLE_SYSTEM_AUDITING=true

      # Enable auditing for all api requests connected to teams.
      CONCOURSE_ENABLE_TEAM_AUDITING=true

      # Enable auditing for all api requests connected to workers.
      CONCOURSE_ENABLE_WORKER_AUDITING=true

      # Enable auditing for all api requests connected to volumes.
      CONCOURSE_ENABLE_VOLUME_AUDITING=true
    }}}

    When enabled, API requests will result in an info-level log line like so:

    \codeblock{json}{{{
    {"timestamp":"2019-05-09T14:41:54.880381537Z","level":"info","source":"atc","message":"atc.audit","data":{"action":"Info","parameters":{},"user":"test"}}
    {"timestamp":"2019-05-09T14:42:36.704864093Z","level":"info","source":"atc","message":"atc.audit","data":{"action":"GetPipeline","parameters":{":pipeline_name":["booklit"],":team_name":["main"]},"user":"test"}}
    }}}
  }
}
