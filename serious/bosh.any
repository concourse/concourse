#language anatomy

\title{Deploying with BOSH}{deploying-with-bosh}

Once you start needing more workers and \italic{really} caring about your CI
deployment, it's best to manage it with BOSH proper.

Using BOSH gives you a self-healing, predictable environment, that can be
scaled up or down just by changing numbers in your deployment manifest.

The \hyperlink{https://bosh.io/docs}{BOSH documentation} outlines the
concepts, and how to bootstrap on various infrastructures.

\table-of-contents

\section{Setting up the infrastructure}{
  Step one is to pick your infrastructure. AWS, vSphere, and OpenStack are
  fully supported by BOSH.
  \hyperlink{https://github.com/cloudfoundry/bosh-lite}{BOSH-Lite} is a
  pseudo-infrastructure that deploys everything within a single VM. It is a
  great way to get started with Concourse and BOSH at the same time, with a
  faster feedback loop.

  Concourse's infrastructure requirements are fairly straightforward. For
  example, Concourse's own pipeline is deployed within an AWS VPC, with its web
  instances automatically registered with an Elastic Load Balancer by BOSH.

  \hyperlink{http://consul.io}{Consul} is baked into the BOSH release, so that
  you only need to configure static IPs for the Consul servers, and then
  configure the Consul agents on the other jobs to join with the server. This
  way you can have 100 workers without having to configure them 100 times.

  \section{BOSH-Lite}{
    Learning BOSH and Concourse at the same time will probably be hard. If
    you're getting started with BOSH, you may want to check out
    \hyperlink{https://github.com/cloudfoundry/bosh-lite}{BOSH-Lite} first,
    which gives you a fairly BOSHy experience, in a single VM on your machine.
    This is a good way to learn the BOSH tooling without having to pay for
    costly infrastructure.
  }

  \section{AWS}{
    For AWS, it is recommended to deploy Concourse within a VPC, with the
    \code{web} jobs sitting behind an ELB. Registering instances with the ELB
    is automated by BOSH; you'll just have to create the ELB itself. This
    configuration is more secure, as your CI system's internal jobs aren't
    exposed to the outside world.

    To get this all set up, follow along with the
    \hyperlink{http://bosh.io/docs/init-aws.html}{\code{bosh-init} AWS docs}.
    You may also want to use our
    \hyperlink{https://github.com/concourse/concourse/blob/master/manifests/cloudformation.json}{CloudFormation
    template}, which will set up a VPC with an ELB and an elastic IP for a BOSH
    director, with appropriate security groups/subnets.
  }

  \section{vSphere, OpenStack, etc.}{
    As long as BOSH supports your infrastructure, you should be able to deploy
    Concourse to it, in largely the same way.

    To get everything set up, reference the \code{bosh-init} bootstrapping docs
    for your infrastructure at \hyperlink{http://bosh.io/docs}{bosh.io}.
  }
}


\section{Deploying Concourse}{
  Once you've set up BOSH on your infrastructure, the following steps should
  get you started:

  \section{Upload the stemcell}{
    A stemcell is a base image for your VMs. It controls the kernel and OS
    distribution, and the version of the BOSH agent. For more information,
    consult the \hyperlink{http://bosh.io/docs/stemcell.html}{BOSH
    documentation}.

    To upload the latest AWS Trusty stemcell to your BOSH director, execute the
    following:

    \codeblock{bash}{
    bosh upload stemcell https://bosh.io/d/stemcells/bosh-aws-xen-hvm-ubuntu-trusty-go_agent
    }
  }

  \section{Upload the releases}{
    A release is a curated distribution of all of the source bits and
    configuration necessary to deploy and scale a product. To learn more about
    BOSH releases, refer to the
    \hyperlink{http://bosh.io/docs/release.html}{BOSH documentation}.

    A Concourse deployment currently requires two releases:
    \hyperlink{http://github.com/concourse/concourse}{Concourse}'s release
    itself, and
    \hyperlink{http://github.com/cloudfoundry-incubator/garden-linux-release}{Garden
    Linux}, which it uses for container management.

    These are two independently-versioned products, so your best bet for
    deploying the correct version combination is to go to our
    \hyperlink{https://github.com/concourse/concourse/releases/latest}{latest
    GitHub release} and grab Concourse and Garden-Linux from the
    \italic{Downloads} section.

    For example, to upload v0.54.0 of Concourse with the appropriate Garden
    Linux release, you would run:

    \codeblock{bash}{
    bosh upload release https://github.com/concourse/concourse/releases/download/v0.54.0/concourse-0.54.0.tgz
    bosh upload release https://github.com/concourse/concourse/releases/download/v0.54.0/garden-linux-0.205.0.tgz
    }
  }

  \section{Configure & Deploy}{bosh-properties}{
    All you need to deploy your entire Concourse cluster is a BOSH deployment
    manifest. This single document describes the desired layout of an entire
    cluster.

    The Concourse repo contains a few example manifests:

    \list{
      \hyperlink{https://github.com/concourse/concourse/blob/master/manifests/bosh-lite.yml}{BOSH Lite}
    }{
      \hyperlink{https://github.com/concourse/concourse/blob/master/manifests/aws-vpc.yml}{AWS VPC}
    }

    If you reuse these manifests, you'll probably want to change the following
    values:

    \definitions{
      \item{\code{director_uuid}}{
        The UUID of your deployment's BOSH director. Obtain this with
        \code{bosh status --uuid}. This is a safeguard against deploying to the
        wrong environments (the risk of making deploys so automated.)
      }

      \item{\code{networks}}{
        Your infrastructure's IP ranges and such will probably be different,
        but may end up being the same if you're using AWS with a VPC that's the
        same CIDR block.
      }

      \item{\code{jobs.discovery.networks.X.static_ips}}{
        A set of static IPs within your network, used for each of the Consul
        servers.

        When changing this, be careful to update each job's
        \code{consul.agent.servers.lan} property to include the full set of
        IPs.

        For a small-scale deployment, a single Consul server is probably fine.
        For HA, you'll probably want to scale up to 3 or so. Refer to
        \hyperlink{http://consul.io}{Consul}'s documentation for more
        information.
      }

      \item{\code{jobs.web.properties.atc.basic_auth_username}, \code{jobs.web.properties.atc.basic_auth_password}}{
        Basic auth credentials for the public web server.
      }

      \item{\code{jobs.web.properties.tsa.atc.username}, \code{jobs.web.properties.tsa.atc.password}}{
        Basic auth credentials to use when registering the worker. These must
        be specified if basic auth is configured, otherwise no workers will be
        available.
      }

      \item{\code{jobs.web.properties.atc.publicly_viewable}}{
        Set to \code{true} to make the webserver open (read-only) to the
        public. Destructive operations are not permitted sensitive data is
        hidden when unauthenticated.
      }

      \item{\code{jobs.db.properties.postgresql.roles}, \code{jobs.web.properties.atc.postgresql.role}}{
        The credentials to the PostgreSQL instance. Note that if you have your
        own PostgreSQL instance somewhere (e.g. RDS), you can just exclude the
        \code{postgresql} job entirely and point the \code{atc} at your own
        instance instead.
      }

      \item{\code{jobs.db.persistent_disk}}{
        How much space to give PostgreSQL. You can change this at any time;
        BOSH will safely migrate your persistent data to a new disk when
        scaling up.
      }

      \item{\code{jobs.worker.instances}}{
        Change this number to scale up or down the number of worker VMs.
        Concourse will randomly pick a VM out of this pool every time it starts
        a build.
      }

      \item{\code{jobs.worker.properties.garden.btrfs_store_size_mb}}{
        The amount of disk space to set aside for use by containers running on
        the worker. If you're running low on disk, bump this number, deploy,
        and recreate your workers.
      }

      \item{\code{resource_pools}}{
        This is where you configure things like your EC2 instance type, the ELB
        to register your instances in, etc.
      }
    }

    Once you have a deployment manifest, deploying Concourse should simply be:

    \codeblock{bash}{
    $ bosh deployment path/to/manifest.yml
    $ bosh deploy
    }
  }
}

\section{Upgrading & Maintenance}{
  Compared to other deployment schemes, the main strength of BOSH its its
  ability to upgrade arbitrary clusters and keep them healthy after the initial
  deploy, even as your infrastructure may be experiencing some turbulence.

  \section{Updating the BOSH deployment}{
    When new Concourse versions come out, upgrading should simply be a matter of
    uploading the new releases
    \hyperlink{https://github.com/concourse/concourse/releases/latest}{from
      GitHub} and \code{bosh deploy}ing again. BOSH will then kick off a rolling
    deploy of your cluster. Be sure to read the \reference{release-notes} first
    though!

    You can change the values in your manifest (such as the number of workers)
    at any time and \code{bosh deploy}, and BOSH will do The Right Thingâ„¢. It
    will tear down VMs as necessary, but always make sure any persistent data
    persists, and that everything's in its right place at the end.
  }

  \section{Self-upgrading Concourse}{
    You can also use Concourse to upgrade itself, using the
    \hyperlink{https://github.com/concourse/concourse/blob/master/ci/pipelines/concourse-redeploy.yml}{concourse-redeploy}
    pipeline configuration. Note that the build that's deploying Concourse from
    itself may fail or error, as it may literally tear down the VM running the
    deploy. But don't worry - all the real work is being done by the BOSH
    director, so just keep an eye on it using \code{bosh task}!
  }

  \section{Dealing with turbulence}{
    Things go wrong all the time. BOSH is built to be durable to things like
    faulty VMs or infrastructure-level failures, and should be able to bring
    your cluster back into the desired state.

    This is often done automatically since BOSH health monitors its VMs and
    will resurrect them (if that's enabled). If you see something wrong and
    want to take action on it immediately, you can run \code{bosh cck} to look
    for errors and fix them. You can also run \code{bosh recreate} to tear down
    an instance and bring up a new one, if you suspect it's gone awry.

    If all else fails, the stateless nature of Concourse should make it pretty
    easy to just nuke the site from orbit (\code{bosh delete deployment}) and
    deploy it all over again. You'll lose build history if you're using the
    built-in \code{postgres} server, but everything should start up again once
    things are back.
  }
}
